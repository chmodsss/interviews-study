# C02-S03: Automotive Problem Management System

The second assurance process from C2 was for S3, an automotive management system for the categorization and analysis of problems. S3 was a Microservice-based rewrite of an existing monolith that was replaced and extended piece by piece and consisted of 10 Java services. Architect P3 rated the maturity level of the assurance activities between level 1 and 2, even though the analysis revealed that the explicit nature of the activities was at least worthy of level 2. Architecture guidelines and standardizations existed and were also partially enforced via the build tool (Maven). Test automation (even though integration tests had started very late) and static source code analysis tools were also used and integrated into the central CI/CD pipeline. Merge requests also had to pass small quality gates. Code reviews were not mandatory but were common practice. SonarQube in combination with FindBugs, Checkstyle, and PMD was used to assess code quality. Apart from common SonarQube metrics, test coverage, clone coverage, and defect resolution time were important. In general, metrics would not play an important role, which P3 saw as improvable. With respect to patterns, *Event-Driven Messaging* (Kafka) was seen as a convenient way to decouple services. Additionally, the *Strangler* pattern was used to continuously extend the existing monolith with new services until its final phase-out. Due to the complexity and importance of the old system, replacing it at once was seen as too risky. P3 generally perceived the evolvability of the system as positive (+1). Reusability would be good because some libraries were shared between the fairly homogeneous services, even though they were aware that this is commonly avoided with Microservices. Because first service cuts were rather technical the modifiability and modularity of the system would not be ideal and changes sometimes caused ripple effects to other services. Replacing of services would also not be so easy due to their relatively large size. In general, the influence of Microservices on the assurance process was seen as slightly negative. There would be an additional layer for testing (integration) and analyzing issues at runtime would be difficult. Distributed code repositories were not really an issue due to automation, but a lot of effort went into the CI/CD pipeline to work around this. While code reviews and quality gates were perceived as effective, the overall assurance effectiveness was rated as neutral (0). The impact of assurance activities on productivity was perceived as positive (+1). However, the customer did not allow more efforts for evolvability assurance, even though the team would have liked to refactor some service cuts and wanted to generally invest more time (+1). P3 noted that the delay of refactorings would significantly hinder productivity in the long-term. Other challenges were the complexity of the technical architecture and the used DevOps and Microservices technologies, which were difficult to master. Lastly, P3 wished that they had started earlier with automated integration testing.